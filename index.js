import { Transform } from 'node:stream';
import mic from 'mic';
import fs from 'node:fs/promises';
import { tmpdir } from 'node:os';
import path from 'node:path';
import play from 'play-sound';
import { nodewhisper } from 'nodejs-whisper';
import { ChatOllama } from "@langchain/ollama";
import { createReactAgent } from "langchain/agents";
import { pull } from "langchain/hub";
import { AgentExecutor } from "langchain/agents";
import { DynamicTool } from "@langchain/core/tools";
import { exec } from 'node:child_process';
import { promisify } from 'node:util';

const execAsync = promisify(exec);

const SAMPLE_RATE = 16000;
const SILENCE_TIMEOUT = 2000;
const TEMP_DIR = tmpdir();
const player = play({ players: ['mpg123'] });

// ÐšÐ°ÑÑ‚Ð¾Ð¼Ð½Ñ‹Ð¹ Ð¸Ð½ÑÑ‚Ñ€ÑƒÐ¼ÐµÐ½Ñ‚ Ð´Ð»Ñ Ð°Ð³ÐµÐ½Ñ‚Ð° (Ð·Ð°Ð³Ð»ÑƒÑˆÐºÐ°)
const customTool = new DynamicTool({
  name: "custom_search",
  description: "ÐŸÐ¾Ð¸ÑÐº Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ð¸ (Ð·Ð°Ð³Ð»ÑƒÑˆÐºÐ°)",
  func: async (input) => {
    return `Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚ Ð¿Ð¾Ð¸ÑÐºÐ° Ð´Ð»Ñ "${input}": Ð—Ð´ÐµÑÑŒ Ð¼Ð¾Ð³Ð»Ð° Ð±Ñ‹Ñ‚ÑŒ Ð¿Ð¾Ð»ÐµÐ·Ð½Ð°Ñ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ñ`;
  },
});

// Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð¼Ð¾Ð´ÐµÐ»Ð¸ DeepSeek Ñ‡ÐµÑ€ÐµÐ· Ollama
const llm = new ChatOllama({
  model: "deepseek-r1:1.5b",
  baseUrl: "http://localhost:11434",
  temperature: 0.1,
});

let executor;

// ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° Ð°Ð³ÐµÐ½Ñ‚Ð°
async function setupAgent() {
  const prompt = await pull("hwchase17/react");
  const agent = await createReactAgent({
    llm,
    tools: [customTool],
    prompt,
  });
  executor = AgentExecutor.fromAgentAndTools({
    agent,
    tools: [customTool],
    handleParsingErrors: true,
  });
}

// Ð”ÐµÑ‚ÐµÐºÑ‚Ð¾Ñ€ Ð°ÐºÑ‚Ð¸Ð²Ð½Ð¾ÑÑ‚Ð¸ Ð³Ð¾Ð»Ð¾ÑÐ° (VAD)
function createVoiceDetector() {
  let silenceTimer = null;
  const vad = new Transform({
    transform(chunk, encoding, callback) {
      const energy = calculateEnergy(chunk);
      if (energy > 0.0005) {
        if (silenceTimer) clearTimeout(silenceTimer);
        silenceTimer = setTimeout(() => {
          vad.emit('silence');
        }, SILENCE_TIMEOUT);
      } else if (!silenceTimer) {
        silenceTimer = setTimeout(() => {
          vad.emit('silence');
        }, SILENCE_TIMEOUT);
      }
      this.push(chunk);
      callback();
    }
  });

  function calculateEnergy(chunk) {
    let energy = 0;
    for (let i = 0; i < chunk.length; i += 2) {
      energy += Math.abs(chunk.readInt16LE(i));
    }
    return energy / (chunk.length / 2) / 32767;
  }

  return vad;
}

// Ð¡Ð»ÑƒÑˆÐ°ÐµÐ¼ Ð¼Ð¸ÐºÑ€Ð¾Ñ„Ð¾Ð½
async function listen() {
  return new Promise((resolve, reject) => {
    const micInstance = mic({
      rate: String(SAMPLE_RATE),
      channels: '1',
      device: 'default',
      format: 'S16_LE',
      debug: false,
      exitOnSilence: 0
    });

    const audioChunks = [];
    const vad = createVoiceDetector();
    const audioStream = micInstance.getAudioStream();

    audioStream
      .pipe(vad)
      .on('data', chunk => {
        audioChunks.push(chunk);
      })
      .on('error', reject);

    vad.on('silence', () => {
      micInstance.stop();
    });

    audioStream
      .on('startComplete', () => console.log('ðŸŽ¤ Ð¡Ð»ÑƒÑˆÐ°ÑŽ...'))
      .on('stopComplete', () => {
        resolve(Buffer.concat(audioChunks));
      });

    const timeout = setTimeout(() => {
      micInstance.stop();
      reject(new Error('Ð¢Ð°Ð¹Ð¼Ð°ÑƒÑ‚ Ð·Ð°Ð¿Ð¸ÑÐ¸'));
    }, 30000);

    micInstance.start();

    audioStream.on('stopComplete', () => clearTimeout(timeout));
  });
}

// ÐšÐ¾Ð½Ð²ÐµÑ€Ñ‚Ð°Ñ†Ð¸Ñ Ð°ÑƒÐ´Ð¸Ð¾ Ð² WAV
async function convertToWav(audioBufferOrPath) {
  const tempOutputFile = path.join(TEMP_DIR, `converted_${Date.now()}.wav`);
  let ffmpegCommand;

  if (typeof audioBufferOrPath === 'string') {
    ffmpegCommand = `ffmpeg -i "${audioBufferOrPath}" -ar 16000 -ac 1 -f wav "${tempOutputFile}" -y`;
  } else {
    const tempInputFile = path.join(TEMP_DIR, `raw_${Date.now()}.pcm`);
    await fs.writeFile(tempInputFile, audioBufferOrPath);
    ffmpegCommand = `ffmpeg -f s16le -ar ${SAMPLE_RATE} -ac 1 -i "${tempInputFile}" -ar 16000 -ac 1 -f wav "${tempOutputFile}" -y`;
    await execAsync(ffmpegCommand);
    await fs.unlink(tempInputFile);
    return tempOutputFile;
  }

  await execAsync(ffmpegCommand);
  return tempOutputFile;
}

// Ð¢Ñ€Ð°Ð½ÑÐºÑ€Ð¸Ð¿Ñ†Ð¸Ñ Ð°ÑƒÐ´Ð¸Ð¾ Ñ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒÑŽ Ð»Ð¾ÐºÐ°Ð»ÑŒÐ½Ð¾Ð¹ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Whisper
async function transcribe(audioBufferOrPath) {
  const wavFile = await convertToWav(audioBufferOrPath);
  const textFile = `${wavFile}.txt`; // Ð¤Ð°Ð¹Ð», ÐºÑƒÐ´Ð° Whisper ÑÐ¾Ñ…Ñ€Ð°Ð½ÑÐµÑ‚ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚
  try {
    const result = await nodewhisper(wavFile, {
      modelName: 'base',
      autoDownloadModelName: 'base',
      removeWavFileAfterTranscription: true,
      whisperOptions: {
        outputInText: true,
        language: 'ru',
      },
    });
    // ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼, ÐµÑÑ‚ÑŒ Ð»Ð¸ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚ Ð² result.text
    if (result.text) {
      return result.text;
    }
    // Ð•ÑÐ»Ð¸ result.text Ð¿ÑƒÑÑ‚Ð¾Ð¹, Ñ‡Ð¸Ñ‚Ð°ÐµÐ¼ Ð¸Ð· Ñ„Ð°Ð¹Ð»Ð° .txt
    const transcribedText = await fs.readFile(textFile, 'utf8');
    return transcribedText.trim() || "ÐžÑˆÐ¸Ð±ÐºÐ°: Ñ‚Ñ€Ð°Ð½ÑÐºÑ€Ð¸Ð¿Ñ†Ð¸Ñ Ð½Ðµ ÑƒÐ´Ð°Ð»Ð°ÑÑŒ";
  } catch (err) {
    console.error('ÐžÑˆÐ¸Ð±ÐºÐ° Ñ‚Ñ€Ð°Ð½ÑÐºÑ€Ð¸Ð¿Ñ†Ð¸Ð¸:', err);
    return "ÐžÑˆÐ¸Ð±ÐºÐ°: Ñ‚Ñ€Ð°Ð½ÑÐºÑ€Ð¸Ð¿Ñ†Ð¸Ñ Ð½Ðµ ÑƒÐ´Ð°Ð»Ð°ÑÑŒ";
  } finally {
    // Ð£Ð´Ð°Ð»ÑÐµÐ¼ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ð¹ Ñ‚ÐµÐºÑÑ‚Ð¾Ð²Ñ‹Ð¹ Ñ„Ð°Ð¹Ð», ÐµÑÐ»Ð¸ Ð¾Ð½ ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÐµÑ‚
    await fs.unlink(textFile).catch(() => {});
  }
}

// ÐŸÐ¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð¾Ñ‚Ð²ÐµÑ‚Ð° Ð¾Ñ‚ Ð°Ð³ÐµÐ½Ñ‚Ð° DeepSeek
async function brainAppeal(text) {
  if (!text || text === "ÐžÑˆÐ¸Ð±ÐºÐ°: Ñ‚Ñ€Ð°Ð½ÑÐºÑ€Ð¸Ð¿Ñ†Ð¸Ñ Ð½Ðµ ÑƒÐ´Ð°Ð»Ð°ÑÑŒ") {
    return "Ð˜Ð·Ð²Ð¸Ð½Ð¸Ñ‚Ðµ, Ð½Ðµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ñ€Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ñ‚ÑŒ Ñ€ÐµÑ‡ÑŒ.";
  }

  console.log('Ð“ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÐµÐ¼ Ð¾Ñ‚Ð²ÐµÑ‚...');
  const prompt = [
    { role: "system", content: "ÐžÑ‚Ð²ÐµÑ‡Ð°Ð¹ ÐºÑ€Ð°Ñ‚ÐºÐ¾ Ð¸ Ð¿Ð¾ Ð´ÐµÐ»Ñƒ, Ð±ÐµÐ· Ð»Ð¸ÑˆÐ½Ð¸Ñ… Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ð¹." },
    { role: "user", content: text }
  ];
  const response = await llm.invoke(prompt);
  return response.content;
}

// Ð¡Ð¸Ð½Ñ‚ÐµÐ· Ð¸ Ð²Ð¾ÑÐ¿Ñ€Ð¾Ð¸Ð·Ð²ÐµÐ´ÐµÐ½Ð¸Ðµ Ð¾Ñ‚Ð²ÐµÑ‚Ð° (Ñ‡ÐµÑ€ÐµÐ· API)
async function voice(text) {
  const response = await fetch('https://api.goapi.ai/v1/audio/speech', {
    method: 'POST',
    headers: {
      'Authorization': `Bearer ${process.env.GOAPI_API_KEY}`,
      'Content-Type': 'application/json'
    },
    body: JSON.stringify({
      model: "tts-1",
      input: text,
      voice: "alloy",
      response_format: "mp3",
      speed: 1.3
    })
  });

  const buffer = Buffer.from(await response.arrayBuffer());
  const outputFile = path.join(TEMP_DIR, `response_${Date.now()}.mp3`);
  await fs.writeFile(outputFile, buffer);

  await new Promise((resolve, reject) => {
    player.play(outputFile, (err) => err ? reject(err) : resolve());
  });

  await fs.unlink(outputFile);
}

// ÐžÑÐ½Ð¾Ð²Ð½Ð¾Ð¹ Ñ†Ð¸ÐºÐ»
async function mainLoop() {
  await setupAgent();
  while (true) {
    try {
      // Ð”Ð»Ñ Ñ‚ÐµÑÑ‚Ð¾Ð² Ð² Gitpod Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ Ð·Ð°Ð³Ð»ÑƒÑˆÐºÑƒ
      const audio = "/workspace/InnerEcho/test/audio_2025-02-22_09-29-56.wav"; // Ð—Ð°Ð¼ÐµÐ½Ð¸Ñ‚Ðµ Ð½Ð° Ð²Ð°Ñˆ Ñ„Ð°Ð¹Ð»
      // const audio = await listen(); // Ð Ð°ÑÐºÐ¾Ð¼Ð¼ÐµÐ½Ñ‚Ð¸Ñ€ÑƒÐ¹Ñ‚Ðµ Ð´Ð»Ñ Ñ€ÐµÐ°Ð»ÑŒÐ½Ð¾Ð¹ Ð·Ð°Ð¿Ð¸ÑÐ¸
      const text = await transcribe(audio);
      console.log('Ð¢Ñ€Ð°Ð½ÑÐºÑ€Ð¸Ð¿Ñ†Ð¸Ñ:', text);
      const response = await brainAppeal(text);
      console.log('ÐžÑ‚Ð²ÐµÑ‚ AI:', response);
      await voice(response);
      await new Promise(resolve => setTimeout(resolve, 500));
    } catch (err) {
      console.error('ÐžÑˆÐ¸Ð±ÐºÐ° Ð² Ð³Ð»Ð°Ð²Ð½Ð¾Ð¼ Ñ†Ð¸ÐºÐ»Ðµ:', err);
      await new Promise(resolve => setTimeout(resolve, 1000));
    }
  }
}

mainLoop().catch(console.error);