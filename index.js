import { Transform } from 'node:stream';
import mic from 'mic';
import fs from 'node:fs/promises';
import path from 'node:path';
import { fileURLToPath } from 'url';
import play from 'play-sound';
import { nodewhisper } from 'nodejs-whisper';
import { ChatOllama } from "@langchain/ollama";
import { createReactAgent } from "langchain/agents";
import { pull } from "langchain/hub";
import { AgentExecutor } from "langchain/agents";
import { DynamicTool } from "@langchain/core/tools";
import { exec } from 'node:child_process';
import { promisify } from 'node:util';
import ZonosJS from 'zonosjs';

const __dirname = path.dirname(fileURLToPath(import.meta.url));
const execAsync = promisify(exec);

const SAMPLE_RATE = 16000;
const SILENCE_TIMEOUT = 2000;
const player = play({ players: ['mpg123'] });
const zonosClient = new ZonosJS();

// –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –∞—É–¥–∏–æ—É—Å—Ç—Ä–æ–π—Å—Ç–≤
async function getAudioDevices() {
  try {
    const { stdout } = await execAsync('ffmpeg -f alsa -list_devices true -i dummy 2>&1');
    const lines = stdout.split('\n');
    const inputs = [];
    const outputs = [];
    lines.forEach(line => {
      if (line.includes('[ALSA]') && line.includes('input')) {
        inputs.push(line.trim());
      } else if (line.includes('[ALSA]') && line.includes('output')) {
        outputs.push(line.trim());
      }
    });
    return { inputs, outputs };
  } catch (err) {
    console.error('–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å–ø–∏—Å–∫–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤:', err);
    return { inputs: ['default'], outputs: ['default'] };
  }
}

// –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–∏–∫—Ä–æ—Ñ–æ–Ω–∞
async function testMicrophone(device = 'default') {
  console.log(`–¢–µ—Å—Ç–∏—Ä—É–µ–º –º–∏–∫—Ä–æ—Ñ–æ–Ω: ${device}`);
  const micInstance = mic({
    rate: String(SAMPLE_RATE),
    channels: '1',
    device,
    format: 'S16_LE',
    debug: false,
    exitOnSilence: 0
  });

  const audioChunks = [];
  const vad = createVoiceDetector();
  const audioStream = micInstance.getAudioStream();

  audioStream.pipe(vad).on('data', chunk => audioChunks.push(chunk));

  vad.on('silence', () => micInstance.stop());
  audioStream.on('startComplete', () => console.log('–ó–∞–ø–∏—Å—å –Ω–∞—á–∞–ª–∞—Å—å...'));
  audioStream.on('stopComplete', async () => {
    const audioBuffer = Buffer.concat(audioChunks);
    const testFile = path.join(__dirname, 'test_mic.wav');
    await fs.writeFile(testFile, audioBuffer);
    console.log(`–ó–∞–ø–∏—Å—å –∑–∞–≤–µ—Ä—à–µ–Ω–∞, —Ñ–∞–π–ª —Å–æ—Ö—Ä–∞–Ω—ë–Ω: ${testFile}`);
    await player.play(testFile, err => {
      if (err) console.error('–û—à–∏–±–∫–∞ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è —Ç–µ—Å—Ç–∞:', err);
      else console.log('–¢–µ—Å—Ç –º–∏–∫—Ä–æ—Ñ–æ–Ω–∞ –∑–∞–≤–µ—Ä—à—ë–Ω');
    });
  });

  micInstance.start();
  await new Promise(resolve => setTimeout(resolve, 5000)); // –ó–∞–ø–∏—Å—å 5 —Å–µ–∫—É–Ω–¥
  micInstance.stop();
}

// –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∞—É–¥–∏–æ—É—Å—Ç—Ä–æ–π—Å—Ç–≤
async function setupAudio() {
  const { inputs, outputs } = await getAudioDevices();
  console.log('–î–æ—Å—Ç—É–ø–Ω—ã–µ –º–∏–∫—Ä–æ—Ñ–æ–Ω—ã:', inputs);
  console.log('–î–æ—Å—Ç—É–ø–Ω—ã–µ –¥–∏–Ω–∞–º–∏–∫–∏:', outputs);

  // –í—ã–±–æ—Ä –º–∏–∫—Ä–æ—Ñ–æ–Ω–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –ø–µ—Ä–≤—ã–π –∏–ª–∏ 'default')
  const selectedMic = inputs.length > 0 ? inputs[0].split(' ')[0] : 'default';
  console.log(`–í—ã–±—Ä–∞–Ω –º–∏–∫—Ä–æ—Ñ–æ–Ω: ${selectedMic}`);
  await testMicrophone(selectedMic);

  // –í—ã–±–æ—Ä –¥–∏–Ω–∞–º–∏–∫–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 'default')
  const selectedSpeaker = outputs.length > 0 ? outputs[0].split(' ')[0] : 'default';
  console.log(`–í—ã–±—Ä–∞–Ω –¥–∏–Ω–∞–º–∏–∫: ${selectedSpeaker}`);
  // –¢–µ—Å—Ç –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è —á–µ—Ä–µ–∑ play-sound –Ω–µ —Ç—Ä–µ–±—É–µ—Ç —è–≤–Ω–æ–≥–æ —É–∫–∞–∑–∞–Ω–∏—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞,
  // —Ç–∞–∫ –∫–∞–∫ mpg123 –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–∏—Å—Ç–µ–º–Ω—ã–π –≤—ã–≤–æ–¥ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é

  return { micDevice: selectedMic, speakerDevice: selectedSpeaker };
}


// –ö–∞—Å—Ç–æ–º–Ω—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è –∞–≥–µ–Ω—Ç–∞ (–∑–∞–≥–ª—É—à–∫–∞)
const customTool = new DynamicTool({
  name: "semantic",
  description: "–∑–∞–ø—Ä–æ—Å –∫ –ø–∞–∫–µ—Ç—É —Å–µ–º–∞–Ω—Ç–∏–∫–∏ (–∑–∞–≥–ª—É—à–∫–∞)",
  func: async (input) => `–†–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ–∏—Å–∫–∞ –¥–ª—è "${input}"`,
});


const llm = new ChatOllama({
  model: "deepseek-r1:1.5b",
  baseUrl: "http://localhost:11434",
  temperature: 0.1,
});

let executor;

// –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∞–≥–µ–Ω—Ç–∞
async function setupAgent() {
  const prompt = await pull("hwchase17/react");
  const agent = await createReactAgent({
    llm,
    tools: [customTool],
    prompt,
  });
  executor = AgentExecutor.fromAgentAndTools({
    agent,
    tools: [customTool],
    handleParsingErrors: true,
  });
}


// –î–µ—Ç–µ–∫—Ç–æ—Ä –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –≥–æ–ª–æ—Å–∞ (VAD)
function createVoiceDetector() {
  let silenceTimer = null;
  const vad = new Transform({
    transform(chunk, encoding, callback) {
      const energy = calculateEnergy(chunk);
      if (energy > 0.0005) {
        if (silenceTimer) clearTimeout(silenceTimer);
        silenceTimer = setTimeout(() => vad.emit('silence'), SILENCE_TIMEOUT);
      } else if (!silenceTimer) {
        silenceTimer = setTimeout(() => vad.emit('silence'), SILENCE_TIMEOUT);
      }
      this.push(chunk);
      callback();
    }
  });

  function calculateEnergy(chunk) {
    let energy = 0;
    for (let i = 0; i < chunk.length; i += 2) {
      energy += Math.abs(chunk.readInt16LE(i));
    }
    return energy / (chunk.length / 2) / 32767;
  }

  return vad;
}

// –°–ª—É—à–∞–µ–º –º–∏–∫—Ä–æ—Ñ–æ–Ω
async function listen(micDevice = 'default') {
  return new Promise((resolve, reject) => {
    const micInstance = mic({
      rate: String(SAMPLE_RATE),
      channels: '1',
      device: micDevice,
      format: 'S16_LE',
      debug: false,
      exitOnSilence: 0
    });

    const audioChunks = [];
    const vad = createVoiceDetector();
    const audioStream = micInstance.getAudioStream();

    audioStream
      .pipe(vad)
      .on('data', chunk => audioChunks.push(chunk))
      .on('error', reject);

    vad.on('silence', () => micInstance.stop());

    audioStream
      .on('startComplete', () => console.log('üé§ –°–ª—É—à–∞—é...'))
      .on('stopComplete', () => resolve(Buffer.concat(audioChunks)));

    const timeout = setTimeout(() => {
      micInstance.stop();
      reject(new Error('–¢–∞–π–º–∞—É—Ç –∑–∞–ø–∏—Å–∏'));
    }, 30000);

    micInstance.start();
    audioStream.on('stopComplete', () => clearTimeout(timeout));
  });
}

// –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –∞—É–¥–∏–æ –≤ WAV
async function convertToWav(audioInput) {
  const tempOutputFile = path.join(__dirname, `converted_${Date.now()}.wav`);
  let ffmpegCommand;

  if (typeof audioInput === 'string') {
    ffmpegCommand = `ffmpeg -i "${audioInput}" -ar 16000 -ac 1 -f wav "${tempOutputFile}" -y`;
  } else {
    const tempInputFile = path.join(__dirname, `raw_${Date.now()}.pcm`);
    await fs.writeFile(tempInputFile, audioInput);
    ffmpegCommand = `ffmpeg -f s16le -ar ${SAMPLE_RATE} -ac 1 -i "${tempInputFile}" -ar 16000 -ac 1 -f wav "${tempOutputFile}" -y`;
    await execAsync(ffmpegCommand);
    await fs.unlink(tempInputFile);
    return tempOutputFile;
  }

  try {
    await execAsync(ffmpegCommand);
    const durationCheck = await execAsync(`ffprobe -i "${tempOutputFile}" -show_entries format=duration -v quiet -of csv="p=0"`);
    const duration = parseFloat(durationCheck.stdout);
    if (duration < 1) {
      const paddedFile = path.join(__dirname, `padded_${Date.now()}.wav`);
      await execAsync(`ffmpeg -i "${tempOutputFile}" -af "apad=pad_dur=1" -ar 16000 -ac 1 "${paddedFile}" -y`);
      await fs.unlink(tempOutputFile);
      return paddedFile;
    }
    return tempOutputFile;
  } catch (err) {
    console.error('–û—à–∏–±–∫–∞ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏:', err);
    throw err;
  }
}


async function transcribe(audioBuffer) {
  const wavFile = await convertToWav(audioBuffer);
  const textFile = `${wavFile}.txt`;
  try {
    const result = await nodewhisper(wavFile, {
      modelName: 'base',
      autoDownloadModelName: 'base',
      removeWavFileAfterTranscription: true,
      whisperOptions: {
        outputInText: true,
        language: 'ru',
      },
    });
    if (result.text) return result.text;
    const transcribedText = await fs.readFile(textFile, 'utf8');
    return transcribedText.trim() || "–û—à–∏–±–∫–∞: —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –Ω–µ —É–¥–∞–ª–∞—Å—å";
  } catch (err) {
    console.error('–û—à–∏–±–∫–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏:', err);
    return "–û—à–∏–±–∫–∞: —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –Ω–µ —É–¥–∞–ª–∞—Å—å";
  } finally {
    await fs.unlink(textFile).catch(() => {});
  }
}

// –ü–æ–ª—É—á–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞ –æ—Ç DeepSeek
async function brainAppeal(text) {
  if (!text || text === "–û—à–∏–±–∫–∞: —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –Ω–µ —É–¥–∞–ª–∞—Å—å") {
    return "–ò–∑–≤–∏–Ω–∏—Ç–µ, –Ω–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å —Ä–µ—á—å.";
  }
  console.log('–ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç...');
  const prompt = [
    { role: "system", content: "–û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ –∏ –ø–æ –¥–µ–ª—É, –±–µ–∑ –ª–∏—à–Ω–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π." },
    { role: "user", content: text }
  ];
  const response = await llm.invoke(prompt);
  return response.content;
}

// –î–æ–±–∞–≤–∏–º –Ω–æ–≤—É—é —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —Ñ–∞–π–ª–æ–º –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è –≥–æ–ª–æ—Å–∞
async function appendToVoiceReference(audioBuffer) {
  const voiceHistoryFile = path.join(__dirname, 'voice_history.wav');
  
  try {
    // –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –Ω–æ–≤—É—é –∑–∞–ø–∏—Å—å –≤ WAV
    const newRecordingWav = await convertToWav(audioBuffer);
    
    // –ï—Å–ª–∏ —Ñ–∞–π–ª –∏—Å—Ç–æ—Ä–∏–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –ø—Ä–æ—Å—Ç–æ –∫–æ–ø–∏—Ä—É–µ–º –Ω–æ–≤—É—é –∑–∞–ø–∏—Å—å
    if (!await fs.access(voiceHistoryFile).then(() => true).catch(() => false)) {
      await fs.copyFile(newRecordingWav, voiceHistoryFile);
    } else {
      // –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –∏—Å—Ç–æ—Ä–∏—é —Å –Ω–æ–≤–æ–π –∑–∞–ø–∏—Å—å—é
      const tempFile = path.join(__dirname, `temp_${Date.now()}.wav`);
      await execAsync(`ffmpeg -i "concat:${voiceHistoryFile}|${newRecordingWav}" -acodec copy "${tempFile}"`);
      await fs.rename(tempFile, voiceHistoryFile);
    }
    
    // –û—á–∏—â–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
    await fs.unlink(newRecordingWav).catch(() => {});
    
  } catch (error) {
    console.error('–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ –∏—Å—Ç–æ—Ä–∏–∏ –≥–æ–ª–æ—Å–∞:', error);
  }
}

// –û–±–Ω–æ–≤–ª—è–µ–º —Ñ—É–Ω–∫—Ü–∏—é voice –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω–æ–π –∏—Å—Ç–æ—Ä–∏–∏ –≥–æ–ª–æ—Å–∞
async function voice(text) {
  console.log('–ù–∞—á–∏–Ω–∞–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏—é —Ä–µ—á–∏ –¥–ª—è —Ç–µ–∫—Å—Ç–∞:', text);
  try {
    console.log('–û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –∫ ZonosJS...');
    const voiceHistoryFile = path.join(__dirname, 'voice_history.wav');
    
    // –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—É—é –∏—Å—Ç–æ—Ä–∏—é –≥–æ–ª–æ—Å–∞ –≤–º–µ—Å—Ç–æ –æ–¥–∏–Ω–æ—á–Ω–æ–≥–æ reference.wav
    const audioBuffer = await zonosClient.generateSpeech(text, voiceHistoryFile, 'ru');
    console.log('–ê—É–¥–∏–æ –ø–æ–ª—É—á–µ–Ω–æ, —Ä–∞–∑–º–µ—Ä –±—É—Ñ–µ—Ä–∞:', audioBuffer.length);

    const audioDir = path.join(__dirname, 'audio');
    await fs.mkdir(audioDir, { recursive: true }).catch(() => {}); 

    const outputFile = path.join(audioDir, `response_${Date.now()}.wav`);
    console.log('–°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∞–π–ª:', outputFile);
    await fs.writeFile(outputFile, audioBuffer);
    console.log('–§–∞–π–ª —Å–æ—Ö—Ä–∞–Ω—ë–Ω');

    console.log('–í–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º –∞—É–¥–∏–æ...');
    await new Promise((resolve, reject) => {
      player.play(outputFile, (err) => {
        if (err) {
          console.error('–û—à–∏–±–∫–∞ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è:', err);
          reject(err);
        } else {
          console.log('–í–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ');
          resolve();
        }
      });
    });
  } catch (error) {
    console.error('–û—à–∏–±–∫–∞ –≤ voice:', error.message);
  }
}

// –û–±–Ω–æ–≤–ª—è–µ–º mainLoop –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –Ω–æ–≤—ã—Ö –∑–∞–ø–∏—Å–µ–π –≤ –∏—Å—Ç–æ—Ä–∏—é –≥–æ–ª–æ—Å–∞
async function mainLoop() {
  await setupAgent();
  const audioSetup = await setupAudio();
  const micDevice = audioSetup.micDevice;

  while (true) {
    try {
      const audio = await listen();
      // –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—É—é –∑–∞–ø–∏—Å—å –≤ –∏—Å—Ç–æ—Ä–∏—é –≥–æ–ª–æ—Å–∞
      await appendToVoiceReference(audio);
      
      const text = await transcribe(audio);
      console.log('–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è:', text);
      const response = await brainAppeal(text);
      console.log('–û—Ç–≤–µ—Ç AI:', response);
      await voice(response);
      await new Promise(resolve => setTimeout(resolve, 500));
    } catch (err) {
      console.error('–û—à–∏–±–∫–∞ –≤ –≥–ª–∞–≤–Ω–æ–º —Ü–∏–∫–ª–µ:', err);
      await new Promise(resolve => setTimeout(resolve, 1000));
    }
  }
}

mainLoop().catch(console.error);